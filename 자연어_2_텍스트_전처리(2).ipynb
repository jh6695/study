{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4i3Bc6MVutf0Vy9b2H7ia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jh6695/study/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4_2_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 정수 인코딩\n",
        "\n",
        "각 단어를 정수에 맵핑 하는 전처리 작업\n",
        "\n",
        "주로 빈도수가 높은 순서대로 정렬하고 차례대로 숫자 매핑\n",
        "\n",
        "# 1) 딕셔너리 사용하기"
      ],
      "metadata": {
        "id": "ssV2AobKfDFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "-hhNFCyVHjUt"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n"
      ],
      "metadata": {
        "id": "pap3MNJvgo7J"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 단위로 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYMRzm71guF-",
        "outputId": "3a8027bc-5918-457f-a904-88f2ebab450f"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정제 정규화 토근화\n",
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_word = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "  # 단어 토근화\n",
        "  tokenize_sentence = word_tokenize(sentence)\n",
        "  result = []\n",
        "\n",
        "  for word in tokenize_sentence:\n",
        "    word = word.lower() # 단어 소문자화\n",
        "    if word not in stop_word: #불용어 제거\n",
        "      if len(word) > 2: # 단어수 2개 이하 제거\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word] = 0\n",
        "        vocab[word] += 1\n",
        "  preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rQBhBqyg08S",
        "outputId": "639e0c49-ada7-4973-f86e-65ee3ab418ac"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z10xeUetr3LP",
        "outputId": "b5db90db-4774-47bb-a182-8b7a1a4b02af"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgSdVRyvsQrR",
        "outputId": "ea5afe34-4ed5-40dc-87f0-57fffeeb836a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 높은 순서대로 정리\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x: x[1], reverse= True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU4SeHiJsWeG",
        "outputId": "ef313230-3ef9-4636-84ae-0203b66e8df4"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 높은 빈도수를 가진 단어부터 정수 부여. 1부터 시작\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency > 1 : # 빈도수가 너무 적은 단어 제거\n",
        "    i = i + 1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0c1e8WssfYO",
        "outputId": "64ed3178-b157-404b-b593-896e3adb50cb"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수 상위 n개인 단어만 사용하고 싶은 경우\n",
        "# n = 5일 때\n",
        "vocab_size = 5\n",
        "\n",
        "#인덱스가 5 초과인 단어 제거\n",
        "word_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "#해당 단어에 대한 인덱스 정보 삭제\n",
        "for w in word_frequency:\n",
        "  del word_to_index[w]\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ScMr4LYu5k5",
        "outputId": "40abde86-02d2-4f46-d022-43db1865eae0"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_to_index를 사용해 단어 토큰화가 되어있는 senetneces에 있는 단어를 정수로 바꾸는 작업\n",
        "\n",
        "단어 집합에 존재하지 않는 단어: Out_Of_Vocablary(OOV)\n",
        "\n",
        "OOV라는 단어를 추하고 없는 단어에 OOV 인덱스로 인코딩\n",
        "\n",
        "['barber', 'good', 'person'] -> [1, OOV, 5]"
      ],
      "metadata": {
        "id": "PiP_J3VR1VZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls1j7myK0iqo",
        "outputId": "95d6be43-0491-48df-f201-1f4908ad5d76"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentences의 모든 단어를 매핑되는 정수로 인코딩\n",
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence = []\n",
        "  for word in sentence:\n",
        "    try :\n",
        "      encoded_sentence.append(word_to_index[word]) # 단어 집합에 있는 단어라면 해당 단어의 정수 리턴\n",
        "    except:\n",
        "      encoded_sentence.append(word_to_index['OOV']) # 단어 집합에 없는 단어면 OOV의 정수 리턴\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBW2gB42JD4",
        "outputId": "992aafef-95b3-49d9-a962-4d33d00b4566"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) counter 사용"
      ],
      "metadata": {
        "id": "x7Np4E_M7pYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "i77Bb2Eq7LQL"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbJjCMqJ_eST",
        "outputId": "dbf17c14-3547-4697-eed0-090dcf6f5c3b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합을 만들기 위해 [,] 제거\n",
        "# np.hstack으로도 가능. 수평 결합\n",
        "all_words_list = sum(preprocessed_sentences, [])\n",
        "print(all_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzEZChW6_wox",
        "outputId": "4955feb7-a26c-46ea-8ae0-095f6c024bae"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counter(): 중복 제거하고 빈도수 기록\n",
        "vocab = Counter(all_words_list)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJz9FWofAGR1",
        "outputId": "e8b7031a-4113-40be-f41a-4877f0617870"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVM_fUzKA0nq",
        "outputId": "c6d6b2f1-f13c-49ed-beb8-77438c8c5a77"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most_common(): 상위 빈도수를 가진 단어만 추출\n",
        "# 상위 5개 단어만 집합으로 저장\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgL7-tCIA8aG",
        "outputId": "be2f28b9-5a1c-4fba-9e51-2a2592947d76"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#내림차순 정수 인덱스 부여\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "\n",
        "for (word, frequency) in vocab:\n",
        "  i = i + 1\n",
        "  word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koUoFdV1BTUA",
        "outputId": "3a3b042b-afc2-4f3d-9a36-cc7f37b44574"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) nltk의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "Wv5jMQpCDRo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PITu2YUJCs_C"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = FreqDist(np.hstack(preprocessed_sentences))"
      ],
      "metadata": {
        "id": "K8RL9_exDef5"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXXagNxqDn-X",
        "outputId": "95e4554d-a03b-4dee-d6a1-96029327c89c"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm5LUVX7DwdZ",
        "outputId": "3c10aaae-9aed-4e85-8283-25fa320332b0"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enumerate()로 내림차순 정수 부여\n",
        "word_to_index = {word[0]: index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pqEJQUOECz8",
        "outputId": "0baf0e68-5a87-49d6-83e3-2ed192dbf7ac"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) enumerate() 이해하기\n",
        "\n",
        "순서가 있는 자료형을 입력받아 인덱스를 순차적으로 함께 리턴함"
      ],
      "metadata": {
        "id": "SGb6KKglEyN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = ['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input):\n",
        "  print(\"value: {}, index: {}\".format(value, index) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mbczsWLElni",
        "outputId": "baaaac55-de21-4bdc-83f7-5f4afa54b3c9"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value: a, index: 0\n",
            "value: b, index: 1\n",
            "value: c, index: 2\n",
            "value: d, index: 3\n",
            "value: e, index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 케라스의 텍스트 전처리\n"
      ],
      "metadata": {
        "id": "ELbww_RAGU05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "_JzVxatVFLdu"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "m4YXLyhpMrDY"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit_on_texts() 빈도수 내림차수 정수 부여\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "EklmlMnEMs_z"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#확인하기 - word_index\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-UHOp9zN6tT",
        "outputId": "196f710c-dfaa-45ad-c79c-da9f12182581"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#각 단어 카운트 - word_counts\n",
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REgHEkFFOReJ",
        "outputId": "41833e30-b4b8-4576-bde7-404110ee9b6a"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#입력으로 들어온 코퍼스에 대해 각 단어를 이미 정해진 인덱스로 변환 - texts_to_sequences()\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piW3XXMaPAwT",
        "outputId": "92055f28-c6bd-4bd3-adba-ca81cf243d16"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#빈도수 상위 5개 추출\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용. 0부터 세기 때문에 +1\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "8xOQ6xgaSIlF"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)\n",
        "#적용 안됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXenHslFSbzM",
        "outputId": "64d40299-9a04-47d0-e86f-54ed7bbcc4fa"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#실제 적용\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdefOoRrSLaY",
        "outputId": "c6c1d14c-aa59-42e6-b93f-2d4b61fec15e"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_index와 word_counts에도 상위 5개 만큼의 단어만 남기고 싶다면\n",
        "vocab_size = 5\n",
        "word_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "#인덱스가 5 초과인 단어 제거\n",
        "for word in word_frequency:\n",
        "  del tokenizer.word_index[word]\n",
        "  del tokenizer.word_counts[word]\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YeY-U1aRVeW",
        "outputId": "81693188-8222-42c6-f6bf-87b1cfac58b8"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OOV 추가\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words= vocab_size + 2, oov_token ='OOV') #0부터 시작함과 OOV를 고려해서 +2\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)"
      ],
      "metadata": {
        "id": "4IdJZfKlTx9Z"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#기본적으로 OOV의 인덱스는 1로 지정됨\n",
        "print(tokenizer.word_index['OOV'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEnofkoyUYeh",
        "outputId": "982a76ff-feda-4674-b7ec-5b3b2bb0b520"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#코퍼스에 대해 정수 인코딩\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
        "# OOV - 1, 빈도수 상위 5 - 2~6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoK-tj0U8YO",
        "outputId": "567fca93-e7e1-4c0c-ed55-2c2551789603"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 1. 패딩\n",
        "\n",
        "병렬 연산을 위해 문장들의 길이를 임의로 동일하게 맞추는 작업\n"
      ],
      "metadata": {
        "id": "UPdh2ZiPawmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "jqDPe5J1VQLa"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "ST3sb16Wdlwo"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 집합 만들고 정수 인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOnqYk5Adn7o",
        "outputId": "e23a0863-e176-4db2-ca96-605f31689bbc"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#동일한 길이로 맞추기 위해 가장 긴 문장 길이 계산\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlqsmGQKeBPf",
        "outputId": "e8ab1384-2776-495f-b2e8-d9d57b1eb6a2"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "제로 패딩\n",
        "\n",
        "모든 문장의 길이를 7로 맞추기 위해 7보다 짧은 문장의 인덱스에 0을 넣어서 7로 맞춘다.\n",
        "\n",
        "0은 아무런 의미 없음."
      ],
      "metadata": {
        "id": "KaI_rfJAe_Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in encoded:\n",
        "  while len(sentence) < max_len:\n",
        "    sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "print(padded_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhbo6oX1eTD1",
        "outputId": "9fa6acf7-cf66-4a05-cac5-5b666ecbc0e5"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  0  0  0  0  0]\n",
            " [ 1  8  5  0  0  0  0]\n",
            " [ 1  3  5  0  0  0  0]\n",
            " [ 9  2  0  0  0  0  0]\n",
            " [ 2  4  3  2  0  0  0]\n",
            " [ 3  2  0  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  2  0  0  0  0]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 케라스 전처리 도구로 패딩하기"
      ],
      "metadata": {
        "id": "UfzGd9dTf0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "GY7iIH8Afvsr"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eocoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM7R0enDgCeq",
        "outputId": "83cc136c-b114-4686-a78e-25a85975c820"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5, 0, 0, 0, 0, 0], [1, 8, 5, 0, 0, 0, 0], [1, 3, 5, 0, 0, 0, 0], [9, 2, 0, 0, 0, 0, 0], [2, 4, 3, 2, 0, 0, 0], [3, 2, 0, 0, 0, 0, 0], [1, 4, 6, 0, 0, 0, 0], [1, 4, 6, 0, 0, 0, 0], [1, 4, 2, 0, 0, 0, 0], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded)\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_xK6aP4gZso",
        "outputId": "a8e10772-1290-40f9-acad-8dbdb277c146"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  0  0  0  0  0]\n",
            " [ 1  8  5  0  0  0  0]\n",
            " [ 1  3  5  0  0  0  0]\n",
            " [ 9  2  0  0  0  0  0]\n",
            " [ 2  4  3  2  0  0  0]\n",
            " [ 3  2  0  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  2  0  0  0  0]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과가 동일한지 확인\n",
        "(padded == padded_np).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xpCEFNBgetE",
        "outputId": "5b9cd900-dc70-4fa5-fcf5-ac87d1d92d72"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터의 길이를 지정해서 맞추는 경우"
      ],
      "metadata": {
        "id": "-gjzwgaPhBUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded, maxlen = 5)\n",
        "padded # 앞부터 삭제"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iBaaxoYgtFj",
        "outputId": "5e19d84d-3dc2-4bce-ada4-fd5c677ef08c"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0],\n",
              "       [ 5,  0,  0,  0,  0],\n",
              "       [ 5,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0],\n",
              "       [ 6,  0,  0,  0,  0],\n",
              "       [ 6,  0,  0,  0,  0],\n",
              "       [ 2,  0,  0,  0,  0],\n",
              "       [ 3,  2, 10,  1, 11],\n",
              "       [ 3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded, maxlen=5, truncating = 'post')\n",
        "padded #뒤가 삭제"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74BBvqwdhMZS",
        "outputId": "b2106199-9fe9-4712-9ef3-138d9378c5a7"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0],\n",
              "       [ 3,  2,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10],\n",
              "       [ 1, 12,  3, 13,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0이 아닌 다른 숫자로 패딩\n",
        "#현재 있는 최대 숫자의 +1\n",
        "last_value = len(tokenizer.word_index) + 1\n",
        "print(last_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq4pp8-BhcIQ",
        "outputId": "a7097c10-c3f0-4a00-e78c-a1caac2c4b9b"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded, padding = 'post',  value = last_value)\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r8kaRqoiHUm",
        "outputId": "72309651-ffaf-4dc2-d222-c507045ab2e8"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#원-핫 인코딩\n",
        "\n",
        "단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1을 부여하고, 다른 단어에 0을 부여하는 단어의 벡터 표현 방식. 이렇게 표현된 벡터를 원-핫 벡터라고 한다.\n",
        "\n",
        "처리 과정\n",
        "\n",
        "1. 정수 인코딩\n",
        "\n",
        "2. 표현하고 싶은 단어의 정수를 인덱스로 간수하고 해당 위치에 1 부여. 다른 모든 단어의 인덱스 위치에는 0 부여."
      ],
      "metadata": {
        "id": "9I14lQ8JjQSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDmmF4cTinIZ",
        "outputId": "bfd2fa3a-8c33-45c9-8f7f-dafefb8feb59"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰에 고유 정수 부여\n",
        "word_to_index = {word: index for index, word in enumerate(tokens)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGvGvCdyn6zH",
        "outputId": "3a93513f-0c78-436d-9bdf-e0881b1b73ce"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#원-핫 벡터 만드는 함수 생성\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector = [0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "JVrQxf9ioXAU"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qJfo8DSpxW6",
        "outputId": "cfec1e4a-e22e-4b9c-c24c-6386479ce634"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 케라스를 이용한 원-핫 인코딩"
      ],
      "metadata": {
        "id": "aB3taKRQp8O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "metadata": {
        "id": "8GlkGJQJp2QU"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "1zmprXYqqCDe"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqrc8KvwqVND",
        "outputId": "edc79cc1-b653-4bfc-fbd8-e702b9e0ff8d"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#집합에 포함되는 단어들로 만들어진 문장을 정수 시퀀스로 만들어 보기\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMR26DnCqoH2",
        "outputId": "7426ecf6-ac23-4ee9-beac-f2a52599917d"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 5, 1, 6, 3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to_categorical 원핫인코딩\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzLfholarVdW",
        "outputId": "4e374ac9-6908-4131-ca76-029f6a7ffa7a"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#원-핫 인코딩의 한계\n",
        "\n",
        "- 단어의 개수가 늘어날수록 벡터의 차원이 늘어남. 저장 공간 측면에서 비효율적.\n",
        "- 유사도 표현 불가. 검색 시스템에서 문제됨.\n",
        "\n",
        "-> 이를 해결하기 위해 잠재 의미를 반영한 다차원 공간 벡터화 가능. LSA, HAL, NNLM, RNNLM, Word2Vec, FastText, GloVe 등"
      ],
      "metadata": {
        "id": "HIORYg6krqZF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAzgUOXFrkY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}